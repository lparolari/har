{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            0         1         2         3         4         5         6    \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n            7         8         9    ...  740  741  742  743  744  745  746  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       747  748  749  \n0      0.0  0.0  0.0  \n1      0.0  0.0  0.0  \n2      0.0  0.0  0.0  \n3      0.0  0.0  0.0  \n4      0.0  0.0  0.0  \n...    ...  ...  ...  \n22584  0.0  0.0  0.0  \n22585  0.0  0.0  0.0  \n22586  0.0  0.0  0.0  \n22587  0.0  0.0  0.0  \n22588  0.0  0.0  0.0  \n\n[22589 rows x 750 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_df = pd.read_csv('./datasets/heterogenity/original/dataset_50_2.5.csv', header=None)\n",
    "\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      user    device     gt\n0        a  nexus4_1  stand\n1        a  nexus4_1  stand\n2        a  nexus4_1  stand\n3        a  nexus4_1  stand\n4        a  nexus4_1  stand\n...    ...       ...    ...\n22584    i  s3mini_2   walk\n22585    i  s3mini_2   walk\n22586    i  s3mini_2   walk\n22587    i  s3mini_2   walk\n22588    i  s3mini_2   walk\n\n[22589 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "label_df = pd.read_csv('./datasets/heterogenity/original/dataset_labels_50_2.5.csv')\n",
    "print(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              0         1         2         3         4         5         6  \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n              7         8         9  ...  743  744  745  746  747  748  749  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       user    device     gt  \n0         a  nexus4_1  stand  \n1         a  nexus4_1  stand  \n2         a  nexus4_1  stand  \n3         a  nexus4_1  stand  \n4         a  nexus4_1  stand  \n...     ...       ...    ...  \n22584     i  s3mini_2   walk  \n22585     i  s3mini_2   walk  \n22586     i  s3mini_2   walk  \n22587     i  s3mini_2   walk  \n22588     i  s3mini_2   walk  \n\n[22589 rows x 753 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.concat([data_df,label_df], axis=1)\n",
    "print(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              0         1         2         3         4         5         6  \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n              7         8         9  ...  748  749  user    device  gt_bike  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0     a  nexus4_1        0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0     a  nexus4_1        0   \n...         ...       ...       ...  ...  ...  ...   ...       ...      ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0     i  s3mini_2        0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0     i  s3mini_2        0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0     i  s3mini_2        0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0     i  s3mini_2        0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0     i  s3mini_2        0   \n\n       gt_sit  gt_stairsdown  gt_stairsup  gt_stand  gt_walk  \n0           0              0            0         1        0  \n1           0              0            0         1        0  \n2           0              0            0         1        0  \n3           0              0            0         1        0  \n4           0              0            0         1        0  \n...       ...            ...          ...       ...      ...  \n22584       0              0            0         0        1  \n22585       0              0            0         0        1  \n22586       0              0            0         0        1  \n22587       0              0            0         0        1  \n22588       0              0            0         0        1  \n\n[22589 rows x 758 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_one_hot_df = pd.get_dummies(dataset_df, columns=['gt'])\n",
    "print(dataset_one_hot_df)"
   ]
  },
  {
   "source": [
    "## SPLITTING DATASET IN TRAIN AND TEST SET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN SET\n\tSit:\t\t3694 (20.44%)\n\tStand:\t\t2968 (16.42%)\n\tWalk:\t\t3991 (22.09%)\n\tBike:\t\t2533 (14.02%)\n\tStairs up:\t2735 (15.13%)\n\tStairs down:\t2150 (11.90%)\nVALIDATION SET\n\tSit:\t\t936 (20.72%)\n\tStand:\t\t736 (16.29%)\n\tWalk:\t\t1018 (22.53%)\n\tBike:\t\t623 (13.79%)\n\tStairs up:\t686 (15.18%)\n\tStairs down:\t519 (11.49%)\n"
     ]
    }
   ],
   "source": [
    "# Keep 20% of the data out for validation\n",
    "### START CODE HERE ### (1 line)\n",
    "train_reference_df, val_reference_df = train_test_split(dataset_one_hot_df, test_size=0.2, shuffle=True, random_state=123)\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "def print_dataset_statistics(train_reference_df, val_reference_df):\n",
    "\n",
    "    # Count the elements in the sets\n",
    "    num_train_data_sit = sum(train_reference_df['gt_sit'] == 1)\n",
    "    num_train_data_stand = sum(train_reference_df['gt_stand'] == 1)\n",
    "    num_train_data_walk = sum(train_reference_df['gt_walk'] == 1)\n",
    "    num_train_data_bike = sum(train_reference_df['gt_bike'] == 1)\n",
    "    num_train_data_stairs_up = sum(train_reference_df['gt_stairsup'] == 1)\n",
    "    num_train_data_stairs_down = sum(train_reference_df['gt_stairsdown'] == 1)\n",
    "\n",
    "    num_val_data_sit = sum(val_reference_df['gt_sit'] == 1)\n",
    "    num_val_data_stand = sum(val_reference_df['gt_stand'] == 1)\n",
    "    num_val_data_walk = sum(val_reference_df['gt_walk'] == 1)\n",
    "    num_val_data_bike = sum(val_reference_df['gt_bike'] == 1)\n",
    "    num_val_data_stairs_up = sum(val_reference_df['gt_stairsup'] == 1)\n",
    "    num_val_data_stairs_down = sum(val_reference_df['gt_stairsdown'] == 1)\n",
    "\n",
    "\n",
    "\n",
    "    print('TRAIN SET')\n",
    "    print('\\tSit:\\t\\t{} ({:.2f}%)'.format(num_train_data_sit, 100 * num_train_data_sit / len(train_reference_df)))\n",
    "    print('\\tStand:\\t\\t{} ({:.2f}%)'.format(num_train_data_stand, 100 * num_train_data_stand / len(train_reference_df)))\n",
    "    print('\\tWalk:\\t\\t{} ({:.2f}%)'.format(num_train_data_walk, 100 * num_train_data_walk / len(train_reference_df)))\n",
    "    print('\\tBike:\\t\\t{} ({:.2f}%)'.format(num_train_data_bike, 100 * num_train_data_bike / len(train_reference_df)))\n",
    "    print('\\tStairs up:\\t{} ({:.2f}%)'.format(num_train_data_stairs_up, 100 * num_train_data_stairs_up / len(train_reference_df)))\n",
    "    print('\\tStairs down:\\t{} ({:.2f}%)'.format(num_train_data_stairs_down, 100 * num_train_data_stairs_down / len(train_reference_df)))\n",
    "\n",
    "\n",
    "    print('VALIDATION SET')\n",
    "    print('\\tSit:\\t\\t{} ({:.2f}%)'.format(num_val_data_sit, 100 * num_val_data_sit / len(val_reference_df)))\n",
    "    print('\\tStand:\\t\\t{} ({:.2f}%)'.format(num_val_data_stand, 100 * num_val_data_stand / len(val_reference_df)))\n",
    "    print('\\tWalk:\\t\\t{} ({:.2f}%)'.format(num_val_data_walk, 100 * num_val_data_walk / len(val_reference_df)))\n",
    "    print('\\tBike:\\t\\t{} ({:.2f}%)'.format(num_val_data_bike, 100 * num_val_data_bike / len(val_reference_df)))\n",
    "    print('\\tStairs up:\\t{} ({:.2f}%)'.format(num_val_data_stairs_up, 100 * num_val_data_stairs_up / len(val_reference_df)))\n",
    "    print('\\tStairs down:\\t{} ({:.2f}%)'.format(num_val_data_stairs_down, 100 * num_val_data_stairs_down / len(val_reference_df)))\n",
    "\n",
    "\n",
    "print_dataset_statistics(train_reference_df, val_reference_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         0         1    2        3         4    5         6         7  \\\n6785  -3.0 -3.000000 -3.0 -3.00000 -2.091919 -2.0 -2.829190 -3.000000   \n20108  5.0  5.000000  5.0  5.00000  5.000000  5.0  5.000000  5.000000   \n17854 -5.0 -4.733199 -4.0 -4.00000 -4.000000 -3.0 -3.000000 -3.214111   \n16603  4.0  4.000000  4.0  4.00000  4.000000  4.0  4.000000  4.000000   \n14823 -1.0 -1.000000 -1.0 -1.00000 -1.000000 -1.0 -1.153836 -2.000000   \n...    ...       ...  ...      ...       ...  ...       ...       ...   \n15377 -1.0 -1.000000 -1.0 -1.00000 -1.000000 -1.0 -1.000000 -1.000000   \n21602  5.0  5.000000  5.0  5.00000  5.000000  5.0  5.000000  5.000000   \n17730 -5.0 -5.000000 -5.0 -4.45692 -4.000000 -3.0 -3.998675 -4.000000   \n15725 -1.0 -1.000000 -1.0 -2.00000 -5.176727 -6.0 -7.000000 -5.000000   \n19966  0.0  0.000000  0.0  0.00000  0.000000  0.0  0.000000  0.000000   \n\n              8         9  ...  748  749  user    device  gt_bike  gt_sit  \\\n6785  -4.000000 -4.818164  ...  0.0  0.0     c      s3_2        0       0   \n20108  5.000000  5.000000  ...  0.0  0.0     i  nexus4_1        0       1   \n17854 -3.000000 -3.000000  ...  0.0  0.0     h  nexus4_1        0       0   \n16603  4.000000  4.000000  ...  0.0  0.0     g      s3_2        0       1   \n14823 -1.879435 -1.000000  ...  0.0  0.0     f  s3mini_1        1       0   \n...         ...       ...  ...  ...  ...   ...       ...      ...     ...   \n15377 -1.000000 -1.000000  ...  0.0  0.0     g  nexus4_2        0       0   \n21602  5.000000  5.000000  ...  0.0  0.0     i      s3_2        0       1   \n17730 -4.000000 -3.460956  ...  0.0  0.0     h  nexus4_1        0       0   \n15725 -4.000000 -4.000000  ...  0.0  0.0     g  nexus4_2        0       0   \n19966  0.000000  0.000000  ...  0.0  0.0     i  nexus4_1        0       0   \n\n       gt_stairsdown  gt_stairsup  gt_stand  gt_walk  \n6785               0            0         0        1  \n20108              0            0         0        0  \n17854              1            0         0        0  \n16603              0            0         0        0  \n14823              0            0         0        0  \n...              ...          ...       ...      ...  \n15377              0            0         1        0  \n21602              0            0         0        0  \n17730              0            0         0        1  \n15725              0            1         0        0  \n19966              0            0         1        0  \n\n[18071 rows x 758 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_reference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 6.3         5.6         5.6         2.9         2.76405499  2.76405499\n   2.5         2.4         2.4        10.26614733  1.          0.\n   1.          1.          1.          1.          1.          1.\n   1.          2.          1.          0.          2.          1.\n   1.          1.          1.          1.          1.          1.\n   1.          0.          2.          1.          1.          1.\n   1.          1.          1.          1.        ]\n [ 5.5         5.5         5.5         2.87228132  2.87228132  2.87228132\n   2.5         2.5         2.5         9.52627944  1.          1.\n   1.          1.          1.          1.          1.          1.\n   1.          1.          1.          1.          1.          1.\n   1.          1.          1.          1.          1.          1.\n   1.          1.          1.          1.          1.          1.\n   1.          1.          1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_basic_features(acc_x, acc_y, acc_z):\n",
    "    # prova = np.array(np.apply_along_axis(np.histogram, 1, acc_x)[0]).reshape(2,1)\n",
    "\n",
    "    np_acc_x = np.array(acc_x)\n",
    "    np_acc_y = np.array(acc_y)\n",
    "    np_acc_z = np.array(acc_z)\n",
    "\n",
    "    mean_x = np.expand_dims(np.mean(np_acc_x, axis=1), axis=0).T \n",
    "    mean_y = np.expand_dims(np.mean(np_acc_y, axis=1), axis=0).T \n",
    "    mean_z = np.expand_dims(np.mean(np_acc_z, axis=1), axis=0).T \n",
    "\n",
    "    basic_features = np.concatenate( (\n",
    "        # insert MEANS \n",
    "        mean_x,\n",
    "        mean_y,\n",
    "        mean_z,\n",
    "\n",
    "        # insert STD\n",
    "        np.expand_dims(np.std(np_acc_x, axis=1), axis=0).T, \n",
    "        np.expand_dims(np.std(np_acc_y, axis=1), axis=0).T, \n",
    "        np.expand_dims(np.std(np_acc_z, axis=1), axis=0).T, \n",
    "\n",
    "        # insert sum of thew absolute values\n",
    "        np.expand_dims(np.mean(abs(np_acc_x - mean_x), axis=1), axis=1),\n",
    "        np.expand_dims(np.mean(abs(np_acc_y - mean_y), axis=1), axis=1),\n",
    "        np.expand_dims(np.mean(abs(np_acc_z - mean_z), axis=1), axis=1),\n",
    "\n",
    "        np.expand_dims(np.mean( np.sqrt( np.power(np_acc_x, 2) + np.power(np_acc_y,2) + np.power(np_acc_z, 2) ), axis=1), axis=0).T\n",
    "        \n",
    "    ), axis=1).tolist()\n",
    "\n",
    "    for i in range(0, len(acc_x)):\n",
    "        bins_x, centers_x = np.histogram(acc_x[i], bins=10)\n",
    "        bins_y, centers_y = np.histogram(acc_y[i], bins=10)\n",
    "        bins_z, centers_z = np.histogram(acc_z[i], bins=10)\n",
    "        basic_features[i].extend(bins_x)\n",
    "        basic_features[i].extend(bins_y)\n",
    "        basic_features[i].extend(bins_z)\n",
    "\n",
    "\n",
    "    return basic_features\n",
    "\n",
    "prova_feat = np.array(extract_basic_features( [[1,10,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]] , [[1,3,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]], [[1,3,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]]))\n",
    "\n",
    "print(prova_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Features: {'input_1': <tf.Tensor: shape=(128, 6, 125), dtype=float64, numpy=\narray([[[-3.        , -3.        , -3.        , ..., -5.        ,\n         -5.        , -4.        ],\n        [ 0.        ,  0.        ,  0.        , ..., -0.28190104,\n          0.        ,  0.        ],\n        [ 5.        ,  5.        ,  5.90402832, ...,  6.        ,\n          6.64831543,  7.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 5.        ,  5.        ,  5.        , ...,  5.        ,\n          5.        ,  5.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 8.        ,  8.        ,  8.        , ...,  8.        ,\n          8.        ,  8.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[-5.        , -4.73319869, -4.        , ..., -3.        ,\n         -3.        , -3.        ],\n        [ 2.        ,  1.26680131,  2.        , ..., -1.        ,\n         -1.        , -1.        ],\n        [10.        ,  8.        ,  7.33589681, ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       ...,\n\n       [[-4.        , -4.        , -4.        , ..., -4.        ,\n         -4.        , -4.        ],\n        [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 8.        ,  9.        ,  9.        , ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 3.05450439,  4.        ,  3.        , ...,  4.        ,\n          4.        ,  4.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 9.        ,  9.        ,  9.        , ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 1.        ,  1.        ,  1.        , ...,  1.        ,\n          1.        ,  1.        ],\n        [-2.        , -2.        , -1.99567871, ..., -2.        ,\n         -1.        , -1.        ],\n        [ 7.        ,  7.        ,  7.        , ...,  7.66396484,\n          8.        ,  7.33168945],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.73662109],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]]])>, 'input_2': <tf.Tensor: shape=(128, 40), dtype=float64, numpy=\narray([[-4.69011002e+00, -8.60144423e-02,  7.68005064e+00, ...,\n         8.00000000e+00,  9.00000000e+00,  5.00000000e+00],\n       [ 5.00000000e+00,  0.00000000e+00,  8.00000000e+00, ...,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [-4.84529689e+00,  1.28745553e-01,  7.87250156e+00, ...,\n         2.00000000e+00,  2.00000000e+00,  1.00000000e+00],\n       ...,\n       [-4.00000000e+00,  5.96103322e-01,  8.74624516e+00, ...,\n         1.00000000e+00,  2.00000000e+00,  8.80000000e+01],\n       [ 3.61243781e+00,  0.00000000e+00,  9.00000000e+00, ...,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 8.34721609e-01,  2.59604138e-02,  8.88621802e+00, ...,\n         1.00000000e+00,  8.00000000e+00,  2.00000000e+00]])>}, Basic Feature:[[-4.69011002e+00 -8.60144423e-02  7.68005064e+00 ...  8.00000000e+00\n   9.00000000e+00  5.00000000e+00]\n [ 5.00000000e+00  0.00000000e+00  8.00000000e+00 ...  0.00000000e+00\n   0.00000000e+00  0.00000000e+00]\n [-4.84529689e+00  1.28745553e-01  7.87250156e+00 ...  2.00000000e+00\n   2.00000000e+00  1.00000000e+00]\n ...\n [-4.00000000e+00  5.96103322e-01  8.74624516e+00 ...  1.00000000e+00\n   2.00000000e+00  8.80000000e+01]\n [ 3.61243781e+00  0.00000000e+00  9.00000000e+00 ...  0.00000000e+00\n   0.00000000e+00  0.00000000e+00]\n [ 8.34721609e-01  2.59604138e-02  8.88621802e+00 ...  1.00000000e+00\n   8.00000000e+00  2.00000000e+00]], Target: [[0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 1 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 0 0 1 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 0 1]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(reference_df, batch_size, shuffle, cache_file):\n",
    "    target = reference_df[['gt_sit','gt_stand','gt_walk','gt_bike','gt_stairsup','gt_stairsdown']].values.astype(int).tolist()\n",
    "\n",
    "    # RESHAPING DATAS\n",
    "    np_data = np.array(reference_df.iloc[:,0:750])\n",
    "    np_reshaped_data = np.reshape(np_data, (np_data.shape[0], 6, 125))\n",
    "\n",
    "    np_basic_features = np.array(extract_basic_features(np_data[:, 0:125], np_data[:, 125:250], np_data[:, 250: 375]))\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices( ({\"input_1\": np_reshaped_data, \"input_2\": np_basic_features}, target) )\n",
    "\n",
    "    # Cache dataset\n",
    "    if cache_file:\n",
    "        dataset = dataset.cache(cache_file)\n",
    "\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(target))\n",
    "\n",
    "    # Repeat the dataset indefinitely\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "    # Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "training_dataset = create_dataset(train_reference_df, batch_size=batch_size, shuffle=False, cache_file=None)\n",
    "val_dataset = create_dataset(val_reference_df, batch_size=batch_size, shuffle=False, cache_file=None)\n",
    "\n",
    "for train, targ in training_dataset.take(1):\n",
    "  print ('Features: {}, Basic Feature:{}, Target: {}'.format(train, basic_features, targ))\n",
    "\n",
    "train_steps = int(np.ceil(len(train_reference_df)/batch_size))\n",
    "val_steps = int(np.ceil(len(val_reference_df)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"OurModel\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6, 125)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 6, 196)       392196      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 2, 196)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 392)          0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_8 (TensorFlo [(None, 392)]        0           flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1024)         402432      tf_op_layer_concat_8[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1024)         0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 6)            6150        dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 800,778\n",
      "Trainable params: 800,778\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_shape):\n",
    "\n",
    "    encoder = tf.keras.models.load_model('encoder.h5')\n",
    "\n",
    "    # NOT TRAIN THE MODEL\n",
    "    encoder.trainable = False\n",
    "\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    training_input = tf.keras.Input(shape=input_shape, dtype=tf.float32, name='input_1')\n",
    "    basic_feat_input = tf.keras.Input(shape=40, dtype=tf.float32, name='input_2')\n",
    "\n",
    "    CNN = tf.keras.layers.Conv1D(196, 16, activation='relu', padding='same')(training_input)\n",
    "    CNN = tf.keras.layers.MaxPool1D(4, padding='same')(CNN)\n",
    "    \n",
    "    feautures_CCN = tf.keras.layers.Flatten()(CNN)\n",
    "    \n",
    "    featuers_encoder = encoder(training_input)\n",
    "\n",
    "    # features = tf.concat((feautures_CCN, basic_feat_input, featuers_encoder), 1) \n",
    "\n",
    "    features = tf.concat((feautures_CCN), 1) \n",
    "\n",
    "    FFNN = tf.keras.layers.Dense(1024, activation='relu')(features)\n",
    "    FFNN = tf.keras.layers.Dropout(0.05)(FFNN)\n",
    "    model_output = tf.keras.layers.Dense(6, activation='softmax')(FFNN)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [training_input, basic_feat_input], outputs = model_output, name='OurModel')\n",
    "\n",
    "\n",
    "    # model = tf.keras.Sequential()\n",
    "    # model.add(tf.keras.Input(shape=(6,125)))\n",
    "    # model.add(tf.keras.layers.Conv1D(196, 16, activation='relu', padding='same'))\n",
    "    # model.add(tf.keras.layers.MaxPool1D(4, padding='same'))\n",
    "\n",
    "    # model.add(tf.keras.layers.Flatten())\n",
    "    # model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
    "    # model.add(tf.keras.layers.Dropout(0.05))\n",
    "    # model.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model((6,125))\n",
    "\n",
    "adam_optimizer = tf.keras.optimizers.Adam()\n",
    "loss_funct = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer = adam_optimizer, loss = loss_funct, metrics = [\"accuracy\"])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 1.1429 - accuracy: 0.6596 - val_loss: 0.4738 - val_accuracy: 0.8247\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.4319 - accuracy: 0.8448 - val_loss: 0.3587 - val_accuracy: 0.8915\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.3201 - accuracy: 0.8858 - val_loss: 0.3343 - val_accuracy: 0.8872\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.9055 - val_loss: 0.3167 - val_accuracy: 0.8845\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.2321 - accuracy: 0.9175 - val_loss: 0.2481 - val_accuracy: 0.9169\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.2240 - accuracy: 0.9205 - val_loss: 0.2436 - val_accuracy: 0.9171\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1824 - accuracy: 0.9344 - val_loss: 0.2710 - val_accuracy: 0.9056\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1755 - accuracy: 0.9351 - val_loss: 0.2392 - val_accuracy: 0.9208\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1600 - accuracy: 0.9406 - val_loss: 0.2384 - val_accuracy: 0.9212\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1650 - accuracy: 0.9385 - val_loss: 0.2740 - val_accuracy: 0.9138\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1536 - accuracy: 0.9428 - val_loss: 0.2864 - val_accuracy: 0.9169\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1527 - accuracy: 0.9435 - val_loss: 0.2460 - val_accuracy: 0.9266\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1417 - accuracy: 0.9478 - val_loss: 0.2038 - val_accuracy: 0.9329\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1357 - accuracy: 0.9473 - val_loss: 0.2263 - val_accuracy: 0.9342\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1398 - accuracy: 0.9461 - val_loss: 0.2407 - val_accuracy: 0.9280\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1305 - accuracy: 0.9498 - val_loss: 0.2594 - val_accuracy: 0.9260\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1293 - accuracy: 0.9495 - val_loss: 0.2259 - val_accuracy: 0.9327\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1237 - accuracy: 0.9499 - val_loss: 0.2416 - val_accuracy: 0.9340\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1160 - accuracy: 0.9546 - val_loss: 0.2047 - val_accuracy: 0.9390\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1191 - accuracy: 0.9525 - val_loss: 0.2172 - val_accuracy: 0.9403\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1269 - accuracy: 0.9500 - val_loss: 0.2452 - val_accuracy: 0.9221\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 1s 5ms/step - loss: 0.1278 - accuracy: 0.9500 - val_loss: 0.2311 - val_accuracy: 0.9332\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f02b4401160>"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "model.fit(training_dataset, epochs = 100, steps_per_epoch=train_steps, validation_data=val_dataset, validation_steps=val_steps,  callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}