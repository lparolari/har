{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            0         1         2         3         4         5         6    \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n            7         8         9    ...  740  741  742  743  744  745  746  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       747  748  749  \n0      0.0  0.0  0.0  \n1      0.0  0.0  0.0  \n2      0.0  0.0  0.0  \n3      0.0  0.0  0.0  \n4      0.0  0.0  0.0  \n...    ...  ...  ...  \n22584  0.0  0.0  0.0  \n22585  0.0  0.0  0.0  \n22586  0.0  0.0  0.0  \n22587  0.0  0.0  0.0  \n22588  0.0  0.0  0.0  \n\n[22589 rows x 750 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_df = pd.read_csv('./datasets/heterogenity/original/dataset_50_2.5.csv', header=None)\n",
    "\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      user     model     gt\n0        a  nexus4_1  stand\n1        a  nexus4_1  stand\n2        a  nexus4_1  stand\n3        a  nexus4_1  stand\n4        a  nexus4_1  stand\n...    ...       ...    ...\n22584    i  s3mini_2   walk\n22585    i  s3mini_2   walk\n22586    i  s3mini_2   walk\n22587    i  s3mini_2   walk\n22588    i  s3mini_2   walk\n\n[22589 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "label_df = pd.read_csv('./datasets/heterogenity/original/dataset_labels_50_2.5.csv', names=[\"user\", \"model\", \"gt\"])\n",
    "print(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              0         1         2         3         4         5         6  \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n              7         8         9  ...  743  744  745  746  747  748  749  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       user     model     gt  \n0         a  nexus4_1  stand  \n1         a  nexus4_1  stand  \n2         a  nexus4_1  stand  \n3         a  nexus4_1  stand  \n4         a  nexus4_1  stand  \n...     ...       ...    ...  \n22584     i  s3mini_2   walk  \n22585     i  s3mini_2   walk  \n22586     i  s3mini_2   walk  \n22587     i  s3mini_2   walk  \n22588     i  s3mini_2   walk  \n\n[22589 rows x 753 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.concat([data_df,label_df], axis=1)\n",
    "print(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              0         1         2         3         4         5         6  \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n              7         8         9  ...  748  749  user     model  gt_bike  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0     a  nexus4_1        0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0     a  nexus4_1        0   \n...         ...       ...       ...  ...  ...  ...   ...       ...      ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0     i  s3mini_2        0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0     i  s3mini_2        0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0     i  s3mini_2        0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0     i  s3mini_2        0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0     i  s3mini_2        0   \n\n       gt_sit  gt_stairsdown  gt_stairsup  gt_stand  gt_walk  \n0           0              0            0         1        0  \n1           0              0            0         1        0  \n2           0              0            0         1        0  \n3           0              0            0         1        0  \n4           0              0            0         1        0  \n...       ...            ...          ...       ...      ...  \n22584       0              0            0         0        1  \n22585       0              0            0         0        1  \n22586       0              0            0         0        1  \n22587       0              0            0         0        1  \n22588       0              0            0         0        1  \n\n[22589 rows x 758 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_one_hot_df = pd.get_dummies(dataset_df, columns=['gt'])\n",
    "print(dataset_one_hot_df)"
   ]
  },
  {
   "source": [
    "## SPLITTING DATASET IN TRAIN AND TEST SET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN SET\n\tSit:\t\t3694 (20.44%)\n\tStand:\t\t2968 (16.42%)\n\tWalk:\t\t3991 (22.09%)\n\tBike:\t\t2533 (14.02%)\n\tStairs up:\t2735 (15.13%)\n\tStairs down:\t2150 (11.90%)\nVALIDATION SET\n\tSit:\t\t936 (20.72%)\n\tStand:\t\t736 (16.29%)\n\tWalk:\t\t1018 (22.53%)\n\tBike:\t\t623 (13.79%)\n\tStairs up:\t686 (15.18%)\n\tStairs down:\t519 (11.49%)\n"
     ]
    }
   ],
   "source": [
    "# Keep 20% of the data out for validation\n",
    "### START CODE HERE ### (1 line)\n",
    "train_reference_df, val_reference_df = train_test_split(dataset_one_hot_df, test_size=0.2, shuffle=True, random_state=123)\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "def print_dataset_statistics(train_reference_df, val_reference_df):\n",
    "\n",
    "    # Count the elements in the sets\n",
    "    num_train_data_sit = sum(train_reference_df['gt_sit'] == 1)\n",
    "    num_train_data_stand = sum(train_reference_df['gt_stand'] == 1)\n",
    "    num_train_data_walk = sum(train_reference_df['gt_walk'] == 1)\n",
    "    num_train_data_bike = sum(train_reference_df['gt_bike'] == 1)\n",
    "    num_train_data_stairs_up = sum(train_reference_df['gt_stairsup'] == 1)\n",
    "    num_train_data_stairs_down = sum(train_reference_df['gt_stairsdown'] == 1)\n",
    "\n",
    "    num_val_data_sit = sum(val_reference_df['gt_sit'] == 1)\n",
    "    num_val_data_stand = sum(val_reference_df['gt_stand'] == 1)\n",
    "    num_val_data_walk = sum(val_reference_df['gt_walk'] == 1)\n",
    "    num_val_data_bike = sum(val_reference_df['gt_bike'] == 1)\n",
    "    num_val_data_stairs_up = sum(val_reference_df['gt_stairsup'] == 1)\n",
    "    num_val_data_stairs_down = sum(val_reference_df['gt_stairsdown'] == 1)\n",
    "\n",
    "\n",
    "\n",
    "    print('TRAIN SET')\n",
    "    print('\\tSit:\\t\\t{} ({:.2f}%)'.format(num_train_data_sit, 100 * num_train_data_sit / len(train_reference_df)))\n",
    "    print('\\tStand:\\t\\t{} ({:.2f}%)'.format(num_train_data_stand, 100 * num_train_data_stand / len(train_reference_df)))\n",
    "    print('\\tWalk:\\t\\t{} ({:.2f}%)'.format(num_train_data_walk, 100 * num_train_data_walk / len(train_reference_df)))\n",
    "    print('\\tBike:\\t\\t{} ({:.2f}%)'.format(num_train_data_bike, 100 * num_train_data_bike / len(train_reference_df)))\n",
    "    print('\\tStairs up:\\t{} ({:.2f}%)'.format(num_train_data_stairs_up, 100 * num_train_data_stairs_up / len(train_reference_df)))\n",
    "    print('\\tStairs down:\\t{} ({:.2f}%)'.format(num_train_data_stairs_down, 100 * num_train_data_stairs_down / len(train_reference_df)))\n",
    "\n",
    "\n",
    "    print('VALIDATION SET')\n",
    "    print('\\tSit:\\t\\t{} ({:.2f}%)'.format(num_val_data_sit, 100 * num_val_data_sit / len(val_reference_df)))\n",
    "    print('\\tStand:\\t\\t{} ({:.2f}%)'.format(num_val_data_stand, 100 * num_val_data_stand / len(val_reference_df)))\n",
    "    print('\\tWalk:\\t\\t{} ({:.2f}%)'.format(num_val_data_walk, 100 * num_val_data_walk / len(val_reference_df)))\n",
    "    print('\\tBike:\\t\\t{} ({:.2f}%)'.format(num_val_data_bike, 100 * num_val_data_bike / len(val_reference_df)))\n",
    "    print('\\tStairs up:\\t{} ({:.2f}%)'.format(num_val_data_stairs_up, 100 * num_val_data_stairs_up / len(val_reference_df)))\n",
    "    print('\\tStairs down:\\t{} ({:.2f}%)'.format(num_val_data_stairs_down, 100 * num_val_data_stairs_down / len(val_reference_df)))\n",
    "\n",
    "\n",
    "print_dataset_statistics(train_reference_df, val_reference_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         0         1    2        3         4    5         6         7  \\\n6785  -3.0 -3.000000 -3.0 -3.00000 -2.091919 -2.0 -2.829190 -3.000000   \n20108  5.0  5.000000  5.0  5.00000  5.000000  5.0  5.000000  5.000000   \n17854 -5.0 -4.733199 -4.0 -4.00000 -4.000000 -3.0 -3.000000 -3.214111   \n16603  4.0  4.000000  4.0  4.00000  4.000000  4.0  4.000000  4.000000   \n14823 -1.0 -1.000000 -1.0 -1.00000 -1.000000 -1.0 -1.153836 -2.000000   \n...    ...       ...  ...      ...       ...  ...       ...       ...   \n15377 -1.0 -1.000000 -1.0 -1.00000 -1.000000 -1.0 -1.000000 -1.000000   \n21602  5.0  5.000000  5.0  5.00000  5.000000  5.0  5.000000  5.000000   \n17730 -5.0 -5.000000 -5.0 -4.45692 -4.000000 -3.0 -3.998675 -4.000000   \n15725 -1.0 -1.000000 -1.0 -2.00000 -5.176727 -6.0 -7.000000 -5.000000   \n19966  0.0  0.000000  0.0  0.00000  0.000000  0.0  0.000000  0.000000   \n\n              8         9  ...  748  749  user     model  gt_bike  gt_sit  \\\n6785  -4.000000 -4.818164  ...  0.0  0.0     c      s3_2        0       0   \n20108  5.000000  5.000000  ...  0.0  0.0     i  nexus4_1        0       1   \n17854 -3.000000 -3.000000  ...  0.0  0.0     h  nexus4_1        0       0   \n16603  4.000000  4.000000  ...  0.0  0.0     g      s3_2        0       1   \n14823 -1.879435 -1.000000  ...  0.0  0.0     f  s3mini_1        1       0   \n...         ...       ...  ...  ...  ...   ...       ...      ...     ...   \n15377 -1.000000 -1.000000  ...  0.0  0.0     g  nexus4_2        0       0   \n21602  5.000000  5.000000  ...  0.0  0.0     i      s3_2        0       1   \n17730 -4.000000 -3.460956  ...  0.0  0.0     h  nexus4_1        0       0   \n15725 -4.000000 -4.000000  ...  0.0  0.0     g  nexus4_2        0       0   \n19966  0.000000  0.000000  ...  0.0  0.0     i  nexus4_1        0       0   \n\n       gt_stairsdown  gt_stairsup  gt_stand  gt_walk  \n6785               0            0         0        1  \n20108              0            0         0        0  \n17854              1            0         0        0  \n16603              0            0         0        0  \n14823              0            0         0        0  \n...              ...          ...       ...      ...  \n15377              0            0         1        0  \n21602              0            0         0        0  \n17730              0            0         0        1  \n15725              0            1         0        0  \n19966              0            0         1        0  \n\n[18071 rows x 758 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_reference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 6.3         5.6         5.6         2.9         2.76405499  2.76405499\n   2.5         2.4         2.4        10.26614733  0.5         0.\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         1.          0.5         0.          1.          0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.          1.          0.5         0.5         0.5\n   0.5         0.5         0.5         0.5       ]\n [ 5.5         5.5         5.5         2.87228132  2.87228132  2.87228132\n   2.5         2.5         2.5         9.52627944  0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_basic_features(acc_x, acc_y, acc_z):\n",
    "    # prova = np.array(np.apply_along_axis(np.histogram, 1, acc_x)[0]).reshape(2,1)\n",
    "\n",
    "    np_acc_x = np.array(acc_x)\n",
    "    np_acc_y = np.array(acc_y)\n",
    "    np_acc_z = np.array(acc_z)\n",
    "\n",
    "    mean_x = np.expand_dims(np.mean(np_acc_x, axis=1), axis=0).T \n",
    "    mean_y = np.expand_dims(np.mean(np_acc_y, axis=1), axis=0).T \n",
    "    mean_z = np.expand_dims(np.mean(np_acc_z, axis=1), axis=0).T \n",
    "\n",
    "    basic_features = np.concatenate( (\n",
    "        # insert MEANS \n",
    "        mean_x,\n",
    "        mean_y,\n",
    "        mean_z,\n",
    "\n",
    "        # insert STD\n",
    "        np.expand_dims(np.std(np_acc_x, axis=1), axis=0).T, \n",
    "        np.expand_dims(np.std(np_acc_y, axis=1), axis=0).T, \n",
    "        np.expand_dims(np.std(np_acc_z, axis=1), axis=0).T, \n",
    "\n",
    "        # insert sum of thew absolute values\n",
    "        np.expand_dims(np.mean(abs(np_acc_x - mean_x), axis=1), axis=1),\n",
    "        np.expand_dims(np.mean(abs(np_acc_y - mean_y), axis=1), axis=1),\n",
    "        np.expand_dims(np.mean(abs(np_acc_z - mean_z), axis=1), axis=1),\n",
    "\n",
    "        np.expand_dims(np.mean( np.sqrt( np.power(np_acc_x, 2) + np.power(np_acc_y,2) + np.power(np_acc_z, 2) ), axis=1), axis=0).T\n",
    "        \n",
    "    ), axis=1).tolist()\n",
    "\n",
    "    for i in range(0, len(acc_x)):\n",
    "        bins_x, centers_x = np.histogram(acc_x[i], bins=10)\n",
    "        bins_y, centers_y = np.histogram(acc_y[i], bins=10)\n",
    "        bins_z, centers_z = np.histogram(acc_z[i], bins=10)\n",
    "\n",
    "        basic_features[i].extend(bins_x / len(acc_x))\n",
    "        basic_features[i].extend(bins_y / len(acc_y))\n",
    "        basic_features[i].extend(bins_z / len(acc_z))\n",
    "\n",
    "    return basic_features\n",
    "\n",
    "prova_feat = np.array(extract_basic_features( [[1,10,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]] , [[1,3,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]], [[1,3,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]]))\n",
    "\n",
    "print(prova_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Features: {'input_1': <tf.Tensor: shape=(128, 6, 125), dtype=float64, numpy=\narray([[[-3.        , -3.        , -3.        , ..., -5.        ,\n         -5.        , -4.        ],\n        [ 0.        ,  0.        ,  0.        , ..., -0.28190104,\n          0.        ,  0.        ],\n        [ 5.        ,  5.        ,  5.90402832, ...,  6.        ,\n          6.64831543,  7.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 5.        ,  5.        ,  5.        , ...,  5.        ,\n          5.        ,  5.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 8.        ,  8.        ,  8.        , ...,  8.        ,\n          8.        ,  8.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[-5.        , -4.73319869, -4.        , ..., -3.        ,\n         -3.        , -3.        ],\n        [ 2.        ,  1.26680131,  2.        , ..., -1.        ,\n         -1.        , -1.        ],\n        [10.        ,  8.        ,  7.33589681, ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       ...,\n\n       [[-4.        , -4.        , -4.        , ..., -4.        ,\n         -4.        , -4.        ],\n        [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 8.        ,  9.        ,  9.        , ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 3.05450439,  4.        ,  3.        , ...,  4.        ,\n          4.        ,  4.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 9.        ,  9.        ,  9.        , ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 1.        ,  1.        ,  1.        , ...,  1.        ,\n          1.        ,  1.        ],\n        [-2.        , -2.        , -1.99567871, ..., -2.        ,\n         -1.        , -1.        ],\n        [ 7.        ,  7.        ,  7.        , ...,  7.66396484,\n          8.        ,  7.33168945],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.73662109],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]]])>, 'input_2': <tf.Tensor: shape=(128, 40), dtype=float64, numpy=\narray([[-4.69011002e+00, -8.60144423e-02,  7.68005064e+00, ...,\n         4.42698246e-04,  4.98035527e-04,  2.76686404e-04],\n       [ 5.00000000e+00,  0.00000000e+00,  8.00000000e+00, ...,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [-4.84529689e+00,  1.28745553e-01,  7.87250156e+00, ...,\n         1.10674561e-04,  1.10674561e-04,  5.53372807e-05],\n       ...,\n       [-4.00000000e+00,  5.96103322e-01,  8.74624516e+00, ...,\n         5.53372807e-05,  1.10674561e-04,  4.86968070e-03],\n       [ 3.61243781e+00,  0.00000000e+00,  9.00000000e+00, ...,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 8.34721609e-01,  2.59604138e-02,  8.88621802e+00, ...,\n         5.53372807e-05,  4.42698246e-04,  1.10674561e-04]])>}, Target: [[0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 1 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 0 0 1 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 0 1]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(reference_df, batch_size, shuffle, cache_file, center_data=False):\n",
    "    target = reference_df[['gt_sit','gt_stand','gt_walk','gt_bike','gt_stairsup','gt_stairsdown']].values.astype(int).tolist()\n",
    "\n",
    "    # RESHAPING DATAS\n",
    "    np_data = np.array(reference_df.iloc[:,0:750])\n",
    "    np_reshaped_data = np.reshape(np_data, (np_data.shape[0], 6, 125))\n",
    "\n",
    "    # Data centering\n",
    "    if center_data:\n",
    "        for i in range(len(np_reshaped_data)):\n",
    "            window = np_reshaped_data[i]\n",
    "            means = np.mean(window, axis=1)\n",
    "            np_reshaped_data[i] = np.array(([window[j] - means[j] for j in range(len(means))]))\n",
    "        \n",
    "    \n",
    "    # Extract manual features\n",
    "    np_basic_features = np.array(extract_basic_features(np_data[:, 0:125], np_data[:, 125:250], np_data[:, 250: 375]))\n",
    "\n",
    "    # Create dataset obj\n",
    "    dataset = tf.data.Dataset.from_tensor_slices( ({\"input_1\": np_reshaped_data, \"input_2\": np_basic_features}, target) )\n",
    "\n",
    "    # Cache dataset\n",
    "    if cache_file:\n",
    "        dataset = dataset.cache(cache_file)\n",
    "\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(target))\n",
    "\n",
    "    # Repeat the dataset indefinitely\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "    # Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "training_dataset = create_dataset(train_reference_df, batch_size=batch_size, shuffle=False, cache_file=None)\n",
    "val_dataset = create_dataset(val_reference_df, batch_size=batch_size, shuffle=False, cache_file=None)\n",
    "\n",
    "for train, targ in training_dataset.take(1):\n",
    "  print ('Features: {}, Target: {}'.format(train, targ))\n",
    "\n",
    "train_steps = int(np.ceil(len(train_reference_df)/batch_size))\n",
    "val_steps = int(np.ceil(len(val_reference_df)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"OurModel\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6, 125)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 6, 196)       392196      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 2, 196)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 392)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_4 (TensorFlo [(None, 432)]        0           flatten_4[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         443392      tf_op_layer_concat_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1024)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 6)            6150        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 841,738\n",
      "Trainable params: 841,738\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_shape):\n",
    "\n",
    "    l2_reg = 5e-4\n",
    "\n",
    "    encoder = tf.keras.models.load_model('encoder.h5')\n",
    "\n",
    "    # NOT TRAIN THE MODEL\n",
    "    encoder.trainable = False\n",
    "\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    training_input = tf.keras.Input(shape=input_shape, dtype=tf.float32, name='input_1')\n",
    "    basic_feat_input = tf.keras.Input(shape=40, dtype=tf.float32, name='input_2')\n",
    "\n",
    "    CNN = tf.keras.layers.Conv1D(196, 16, activation='relu', padding='same')(training_input)\n",
    "    CNN = tf.keras.layers.MaxPool1D(4, padding='same')(CNN)\n",
    "    \n",
    "    feautures_CCN = tf.keras.layers.Flatten()(CNN)\n",
    "    \n",
    "    featuers_encoder = encoder(training_input)\n",
    "\n",
    "    features = tf.concat((feautures_CCN, basic_feat_input), 1) \n",
    "\n",
    "    #features = tf.concat((feautures_CCN), 1) \n",
    "\n",
    "    FFNN = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2_reg), activity_regularizer=tf.keras.regularizers.L2(l2_reg))(features)\n",
    "    FFNN = tf.keras.layers.Dropout(0.05)(FFNN)\n",
    "    model_output = tf.keras.layers.Dense(6, activation='softmax')(FFNN)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [training_input, basic_feat_input], outputs = model_output, name='OurModel')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model((6,125))\n",
    "\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "loss_funct = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer = adam_optimizer, loss = loss_funct, metrics = [\"accuracy\"])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 1.2486 - accuracy: 0.7254 - val_loss: 0.7350 - val_accuracy: 0.8663\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.6534 - accuracy: 0.8828 - val_loss: 0.5816 - val_accuracy: 0.9186\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.5156 - accuracy: 0.9191 - val_loss: 0.4851 - val_accuracy: 0.9301\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.4411 - accuracy: 0.9377 - val_loss: 0.4306 - val_accuracy: 0.9371\n",
      "Epoch 5/100\n",
      "136/142 [===========================>..] - ETA: 0s - loss: 0.3918 - accuracy: 0.9469"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d6a39858be1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearly_stopping_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "model.fit(training_dataset, epochs = 100, steps_per_epoch=train_steps, validation_data=val_dataset, validation_steps=val_steps,  callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model.h5')"
   ]
  }
 ]
}