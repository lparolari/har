{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            0         1         2         3         4         5         6    \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n            7         8         9    ...  740  741  742  743  744  745  746  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       747  748  749  \n0      0.0  0.0  0.0  \n1      0.0  0.0  0.0  \n2      0.0  0.0  0.0  \n3      0.0  0.0  0.0  \n4      0.0  0.0  0.0  \n...    ...  ...  ...  \n22584  0.0  0.0  0.0  \n22585  0.0  0.0  0.0  \n22586  0.0  0.0  0.0  \n22587  0.0  0.0  0.0  \n22588  0.0  0.0  0.0  \n\n[22589 rows x 750 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_df = pd.read_csv('./datasets/heterogenity/original/dataset_50_2.5.csv', header=None)\n",
    "\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      user     model     gt\n0        a  nexus4_1  stand\n1        a  nexus4_1  stand\n2        a  nexus4_1  stand\n3        a  nexus4_1  stand\n4        a  nexus4_1  stand\n...    ...       ...    ...\n22584    i  s3mini_2   walk\n22585    i  s3mini_2   walk\n22586    i  s3mini_2   walk\n22587    i  s3mini_2   walk\n22588    i  s3mini_2   walk\n\n[22589 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "label_df = pd.read_csv('./datasets/heterogenity/original/dataset_labels_50_2.5.csv', names=[\"user\", \"model\", \"gt\"])\n",
    "print(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              0         1         2         3         4         5         6  \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n              7         8         9  ...  743  744  745  746  747  748  749  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       user     model     gt  \n0         a  nexus4_1  stand  \n1         a  nexus4_1  stand  \n2         a  nexus4_1  stand  \n3         a  nexus4_1  stand  \n4         a  nexus4_1  stand  \n...     ...       ...    ...  \n22584     i  s3mini_2   walk  \n22585     i  s3mini_2   walk  \n22586     i  s3mini_2   walk  \n22587     i  s3mini_2   walk  \n22588     i  s3mini_2   walk  \n\n[22589 rows x 753 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.concat([data_df,label_df], axis=1)\n",
    "print(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              0         1         2         3         4         5         6  \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n              7         8         9  ...  748  749  user     model  gt_bike  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0     a  nexus4_1        0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0     a  nexus4_1        0   \n...         ...       ...       ...  ...  ...  ...   ...       ...      ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0     i  s3mini_2        0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0     i  s3mini_2        0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0     i  s3mini_2        0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0     i  s3mini_2        0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0     i  s3mini_2        0   \n\n       gt_sit  gt_stairsdown  gt_stairsup  gt_stand  gt_walk  \n0           0              0            0         1        0  \n1           0              0            0         1        0  \n2           0              0            0         1        0  \n3           0              0            0         1        0  \n4           0              0            0         1        0  \n...       ...            ...          ...       ...      ...  \n22584       0              0            0         0        1  \n22585       0              0            0         0        1  \n22586       0              0            0         0        1  \n22587       0              0            0         0        1  \n22588       0              0            0         0        1  \n\n[22589 rows x 758 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_one_hot_df = pd.get_dummies(dataset_df, columns=['gt'])\n",
    "print(dataset_one_hot_df)"
   ]
  },
  {
   "source": [
    "## SPLITTING DATASET IN TRAIN AND TEST SET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN SET\n\tSit:\t\t3694 (20.44%)\n\tStand:\t\t2968 (16.42%)\n\tWalk:\t\t3991 (22.09%)\n\tBike:\t\t2533 (14.02%)\n\tStairs up:\t2735 (15.13%)\n\tStairs down:\t2150 (11.90%)\nVALIDATION SET\n\tSit:\t\t936 (20.72%)\n\tStand:\t\t736 (16.29%)\n\tWalk:\t\t1018 (22.53%)\n\tBike:\t\t623 (13.79%)\n\tStairs up:\t686 (15.18%)\n\tStairs down:\t519 (11.49%)\n"
     ]
    }
   ],
   "source": [
    "# Keep 20% of the data out for validation\n",
    "### START CODE HERE ### (1 line)\n",
    "train_reference_df, val_reference_df = train_test_split(dataset_one_hot_df, test_size=0.2, shuffle=True, random_state=123)\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "def print_dataset_statistics(train_reference_df, val_reference_df):\n",
    "\n",
    "    # Count the elements in the sets\n",
    "    num_train_data_sit = sum(train_reference_df['gt_sit'] == 1)\n",
    "    num_train_data_stand = sum(train_reference_df['gt_stand'] == 1)\n",
    "    num_train_data_walk = sum(train_reference_df['gt_walk'] == 1)\n",
    "    num_train_data_bike = sum(train_reference_df['gt_bike'] == 1)\n",
    "    num_train_data_stairs_up = sum(train_reference_df['gt_stairsup'] == 1)\n",
    "    num_train_data_stairs_down = sum(train_reference_df['gt_stairsdown'] == 1)\n",
    "\n",
    "    num_val_data_sit = sum(val_reference_df['gt_sit'] == 1)\n",
    "    num_val_data_stand = sum(val_reference_df['gt_stand'] == 1)\n",
    "    num_val_data_walk = sum(val_reference_df['gt_walk'] == 1)\n",
    "    num_val_data_bike = sum(val_reference_df['gt_bike'] == 1)\n",
    "    num_val_data_stairs_up = sum(val_reference_df['gt_stairsup'] == 1)\n",
    "    num_val_data_stairs_down = sum(val_reference_df['gt_stairsdown'] == 1)\n",
    "\n",
    "\n",
    "\n",
    "    print('TRAIN SET')\n",
    "    print('\\tSit:\\t\\t{} ({:.2f}%)'.format(num_train_data_sit, 100 * num_train_data_sit / len(train_reference_df)))\n",
    "    print('\\tStand:\\t\\t{} ({:.2f}%)'.format(num_train_data_stand, 100 * num_train_data_stand / len(train_reference_df)))\n",
    "    print('\\tWalk:\\t\\t{} ({:.2f}%)'.format(num_train_data_walk, 100 * num_train_data_walk / len(train_reference_df)))\n",
    "    print('\\tBike:\\t\\t{} ({:.2f}%)'.format(num_train_data_bike, 100 * num_train_data_bike / len(train_reference_df)))\n",
    "    print('\\tStairs up:\\t{} ({:.2f}%)'.format(num_train_data_stairs_up, 100 * num_train_data_stairs_up / len(train_reference_df)))\n",
    "    print('\\tStairs down:\\t{} ({:.2f}%)'.format(num_train_data_stairs_down, 100 * num_train_data_stairs_down / len(train_reference_df)))\n",
    "\n",
    "\n",
    "    print('VALIDATION SET')\n",
    "    print('\\tSit:\\t\\t{} ({:.2f}%)'.format(num_val_data_sit, 100 * num_val_data_sit / len(val_reference_df)))\n",
    "    print('\\tStand:\\t\\t{} ({:.2f}%)'.format(num_val_data_stand, 100 * num_val_data_stand / len(val_reference_df)))\n",
    "    print('\\tWalk:\\t\\t{} ({:.2f}%)'.format(num_val_data_walk, 100 * num_val_data_walk / len(val_reference_df)))\n",
    "    print('\\tBike:\\t\\t{} ({:.2f}%)'.format(num_val_data_bike, 100 * num_val_data_bike / len(val_reference_df)))\n",
    "    print('\\tStairs up:\\t{} ({:.2f}%)'.format(num_val_data_stairs_up, 100 * num_val_data_stairs_up / len(val_reference_df)))\n",
    "    print('\\tStairs down:\\t{} ({:.2f}%)'.format(num_val_data_stairs_down, 100 * num_val_data_stairs_down / len(val_reference_df)))\n",
    "\n",
    "\n",
    "print_dataset_statistics(train_reference_df, val_reference_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         0         1    2        3         4    5         6         7  \\\n6785  -3.0 -3.000000 -3.0 -3.00000 -2.091919 -2.0 -2.829190 -3.000000   \n20108  5.0  5.000000  5.0  5.00000  5.000000  5.0  5.000000  5.000000   \n17854 -5.0 -4.733199 -4.0 -4.00000 -4.000000 -3.0 -3.000000 -3.214111   \n16603  4.0  4.000000  4.0  4.00000  4.000000  4.0  4.000000  4.000000   \n14823 -1.0 -1.000000 -1.0 -1.00000 -1.000000 -1.0 -1.153836 -2.000000   \n...    ...       ...  ...      ...       ...  ...       ...       ...   \n15377 -1.0 -1.000000 -1.0 -1.00000 -1.000000 -1.0 -1.000000 -1.000000   \n21602  5.0  5.000000  5.0  5.00000  5.000000  5.0  5.000000  5.000000   \n17730 -5.0 -5.000000 -5.0 -4.45692 -4.000000 -3.0 -3.998675 -4.000000   \n15725 -1.0 -1.000000 -1.0 -2.00000 -5.176727 -6.0 -7.000000 -5.000000   \n19966  0.0  0.000000  0.0  0.00000  0.000000  0.0  0.000000  0.000000   \n\n              8         9  ...  748  749  user     model  gt_bike  gt_sit  \\\n6785  -4.000000 -4.818164  ...  0.0  0.0     c      s3_2        0       0   \n20108  5.000000  5.000000  ...  0.0  0.0     i  nexus4_1        0       1   \n17854 -3.000000 -3.000000  ...  0.0  0.0     h  nexus4_1        0       0   \n16603  4.000000  4.000000  ...  0.0  0.0     g      s3_2        0       1   \n14823 -1.879435 -1.000000  ...  0.0  0.0     f  s3mini_1        1       0   \n...         ...       ...  ...  ...  ...   ...       ...      ...     ...   \n15377 -1.000000 -1.000000  ...  0.0  0.0     g  nexus4_2        0       0   \n21602  5.000000  5.000000  ...  0.0  0.0     i      s3_2        0       1   \n17730 -4.000000 -3.460956  ...  0.0  0.0     h  nexus4_1        0       0   \n15725 -4.000000 -4.000000  ...  0.0  0.0     g  nexus4_2        0       0   \n19966  0.000000  0.000000  ...  0.0  0.0     i  nexus4_1        0       0   \n\n       gt_stairsdown  gt_stairsup  gt_stand  gt_walk  \n6785               0            0         0        1  \n20108              0            0         0        0  \n17854              1            0         0        0  \n16603              0            0         0        0  \n14823              0            0         0        0  \n...              ...          ...       ...      ...  \n15377              0            0         1        0  \n21602              0            0         0        0  \n17730              0            0         0        1  \n15725              0            1         0        0  \n19966              0            0         1        0  \n\n[18071 rows x 758 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_reference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 6.3         5.6         5.6         2.9         2.76405499  2.76405499\n   2.5         2.4         2.4        10.26614733  0.5         0.\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         1.          0.5         0.          1.          0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.          1.          0.5         0.5         0.5\n   0.5         0.5         0.5         0.5       ]\n [ 5.5         5.5         5.5         2.87228132  2.87228132  2.87228132\n   2.5         2.5         2.5         9.52627944  0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_basic_features(acc_x, acc_y, acc_z):\n",
    "    # prova = np.array(np.apply_along_axis(np.histogram, 1, acc_x)[0]).reshape(2,1)\n",
    "\n",
    "    np_acc_x = np.array(acc_x)\n",
    "    np_acc_y = np.array(acc_y)\n",
    "    np_acc_z = np.array(acc_z)\n",
    "\n",
    "    mean_x = np.expand_dims(np.mean(np_acc_x, axis=1), axis=0).T \n",
    "    mean_y = np.expand_dims(np.mean(np_acc_y, axis=1), axis=0).T \n",
    "    mean_z = np.expand_dims(np.mean(np_acc_z, axis=1), axis=0).T \n",
    "\n",
    "    basic_features = np.concatenate( (\n",
    "        # insert MEANS \n",
    "        mean_x,\n",
    "        mean_y,\n",
    "        mean_z,\n",
    "\n",
    "        # insert STD\n",
    "        np.expand_dims(np.std(np_acc_x, axis=1), axis=0).T, \n",
    "        np.expand_dims(np.std(np_acc_y, axis=1), axis=0).T, \n",
    "        np.expand_dims(np.std(np_acc_z, axis=1), axis=0).T, \n",
    "\n",
    "        # insert sum of thew absolute values\n",
    "        np.expand_dims(np.mean(abs(np_acc_x - mean_x), axis=1), axis=1),\n",
    "        np.expand_dims(np.mean(abs(np_acc_y - mean_y), axis=1), axis=1),\n",
    "        np.expand_dims(np.mean(abs(np_acc_z - mean_z), axis=1), axis=1),\n",
    "\n",
    "        np.expand_dims(np.mean( np.sqrt( np.power(np_acc_x, 2) + np.power(np_acc_y,2) + np.power(np_acc_z, 2) ), axis=1), axis=0).T\n",
    "        \n",
    "    ), axis=1).tolist()\n",
    "\n",
    "    for i in range(0, len(acc_x)):\n",
    "        bins_x, centers_x = np.histogram(acc_x[i], bins=10)\n",
    "        bins_y, centers_y = np.histogram(acc_y[i], bins=10)\n",
    "        bins_z, centers_z = np.histogram(acc_z[i], bins=10)\n",
    "\n",
    "        basic_features[i].extend(bins_x / len(acc_x))\n",
    "        basic_features[i].extend(bins_y / len(acc_y))\n",
    "        basic_features[i].extend(bins_z / len(acc_z))\n",
    "\n",
    "    return basic_features\n",
    "\n",
    "prova_feat = np.array(extract_basic_features( [[1,10,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]] , [[1,3,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]], [[1,3,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]]))\n",
    "\n",
    "print(prova_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Features: {'input_1': <tf.Tensor: shape=(128, 6, 125), dtype=float64, numpy=\narray([[[-3.        , -3.        , -3.        , ..., -5.        ,\n         -5.        , -4.        ],\n        [ 0.        ,  0.        ,  0.        , ..., -0.28190104,\n          0.        ,  0.        ],\n        [ 5.        ,  5.        ,  5.90402832, ...,  6.        ,\n          6.64831543,  7.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 5.        ,  5.        ,  5.        , ...,  5.        ,\n          5.        ,  5.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 8.        ,  8.        ,  8.        , ...,  8.        ,\n          8.        ,  8.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[-5.        , -4.73319869, -4.        , ..., -3.        ,\n         -3.        , -3.        ],\n        [ 2.        ,  1.26680131,  2.        , ..., -1.        ,\n         -1.        , -1.        ],\n        [10.        ,  8.        ,  7.33589681, ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       ...,\n\n       [[-4.        , -4.        , -4.        , ..., -4.        ,\n         -4.        , -4.        ],\n        [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 8.        ,  9.        ,  9.        , ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 3.05450439,  4.        ,  3.        , ...,  4.        ,\n          4.        ,  4.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 9.        ,  9.        ,  9.        , ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 1.        ,  1.        ,  1.        , ...,  1.        ,\n          1.        ,  1.        ],\n        [-2.        , -2.        , -1.99567871, ..., -2.        ,\n         -1.        , -1.        ],\n        [ 7.        ,  7.        ,  7.        , ...,  7.66396484,\n          8.        ,  7.33168945],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.73662109],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]]])>, 'input_2': <tf.Tensor: shape=(128, 40), dtype=float64, numpy=\narray([[-4.69011002e+00, -8.60144423e-02,  7.68005064e+00, ...,\n         4.42698246e-04,  4.98035527e-04,  2.76686404e-04],\n       [ 5.00000000e+00,  0.00000000e+00,  8.00000000e+00, ...,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [-4.84529689e+00,  1.28745553e-01,  7.87250156e+00, ...,\n         1.10674561e-04,  1.10674561e-04,  5.53372807e-05],\n       ...,\n       [-4.00000000e+00,  5.96103322e-01,  8.74624516e+00, ...,\n         5.53372807e-05,  1.10674561e-04,  4.86968070e-03],\n       [ 3.61243781e+00,  0.00000000e+00,  9.00000000e+00, ...,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 8.34721609e-01,  2.59604138e-02,  8.88621802e+00, ...,\n         5.53372807e-05,  4.42698246e-04,  1.10674561e-04]])>}, Target: [[0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 1 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 0 0 1 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 0 1]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(reference_df, batch_size, shuffle, cache_file):\n",
    "    target = reference_df[['gt_sit','gt_stand','gt_walk','gt_bike','gt_stairsup','gt_stairsdown']].values.astype(int).tolist()\n",
    "\n",
    "    # RESHAPING DATAS\n",
    "    np_data = np.array(reference_df.iloc[:,0:750])\n",
    "    np_reshaped_data = np.reshape(np_data, (np_data.shape[0], 6, 125))\n",
    "\n",
    "    # Data centering\n",
    "    for window in np_reshaped_data:\n",
    "        means = np.mean(window, axis=1)\n",
    "        window = np.array([(window[i] - means[i]) for i in range(len(means))])\n",
    "    \n",
    "    # Extract manual features\n",
    "    np_basic_features = np.array(extract_basic_features(np_data[:, 0:125], np_data[:, 125:250], np_data[:, 250: 375]))\n",
    "\n",
    "    # Create dataset obj\n",
    "    dataset = tf.data.Dataset.from_tensor_slices( ({\"input_1\": np_reshaped_data, \"input_2\": np_basic_features}, target) )\n",
    "\n",
    "    # Cache dataset\n",
    "    if cache_file:\n",
    "        dataset = dataset.cache(cache_file)\n",
    "\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(target))\n",
    "\n",
    "    # Repeat the dataset indefinitely\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "    # Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "training_dataset = create_dataset(train_reference_df, batch_size=batch_size, shuffle=False, cache_file=None)\n",
    "val_dataset = create_dataset(val_reference_df, batch_size=batch_size, shuffle=False, cache_file=None)\n",
    "\n",
    "for train, targ in training_dataset.take(1):\n",
    "  print ('Features: {}, Target: {}'.format(train, targ))\n",
    "\n",
    "train_steps = int(np.ceil(len(train_reference_df)/batch_size))\n",
    "val_steps = int(np.ceil(len(val_reference_df)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"OurModel\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6, 125)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 6, 196)       392196      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 2, 196)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 392)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_2 (TFOpLambda)        (None, 432)          0           flatten_2[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         443392      tf.concat_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 6)            6150        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 841,738\n",
      "Trainable params: 841,738\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_shape):\n",
    "\n",
    "    l2_reg = 5e-4\n",
    "\n",
    "    encoder = tf.keras.models.load_model('encoder.h5')\n",
    "\n",
    "    # NOT TRAIN THE MODEL\n",
    "    encoder.trainable = False\n",
    "\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    training_input = tf.keras.Input(shape=input_shape, dtype=tf.float32, name='input_1')\n",
    "    basic_feat_input = tf.keras.Input(shape=40, dtype=tf.float32, name='input_2')\n",
    "\n",
    "    CNN = tf.keras.layers.Conv1D(196, 16, activation='relu', padding='same')(training_input)\n",
    "    CNN = tf.keras.layers.MaxPool1D(4, padding='same')(CNN)\n",
    "    \n",
    "    feautures_CCN = tf.keras.layers.Flatten()(CNN)\n",
    "    \n",
    "    featuers_encoder = encoder(training_input)\n",
    "\n",
    "    features = tf.concat((feautures_CCN, basic_feat_input), 1) \n",
    "\n",
    "    #features = tf.concat((feautures_CCN), 1) \n",
    "\n",
    "    FFNN = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2_reg), activity_regularizer=tf.keras.regularizers.L2(l2_reg))(features)\n",
    "    FFNN = tf.keras.layers.Dropout(0.05)(FFNN)\n",
    "    model_output = tf.keras.layers.Dense(6, activation='softmax')(FFNN)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [training_input, basic_feat_input], outputs = model_output, name='OurModel')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model((6,125))\n",
    "\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "loss_funct = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer = adam_optimizer, loss = loss_funct, metrics = [\"accuracy\"])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 15ms/step - loss: 1.8490 - accuracy: 0.5854 - val_loss: 0.7385 - val_accuracy: 0.8598\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.7000 - accuracy: 0.8683 - val_loss: 0.5711 - val_accuracy: 0.9128\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 0.5316 - accuracy: 0.9174 - val_loss: 0.4830 - val_accuracy: 0.9334\n",
      "Epoch 4/100\n",
      "140/142 [============================>.] - ETA: 0s - loss: 0.4456 - accuracy: 0.9362"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-d6a39858be1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearly_stopping_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/har/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1131\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1132\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/har/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;31m# Use cached evaluation data only when it's called in `Model.fit`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m       if (getattr(self, '_fit_frame', None) is not None\n\u001b[0;32m-> 1349\u001b[0;31m           \u001b[0;32mand\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m           and getattr(self, '_eval_data_handler', None) is not None):\n\u001b[1;32m   1351\u001b[0m         \u001b[0mdata_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_data_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/har/.venv/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_inspect.py\u001b[0m in \u001b[0;36mcurrentframe\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;34m\"\"\"TFDecorator-aware replacement for inspect.currentframe.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;34m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetouterframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[0mframelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m         \u001b[0mframeinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgetframeinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0mframelist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrameInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mframeinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1463\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'source code not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;31m# Copy sys.modules in order to cope with changes while iterating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mismodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_filesbymodname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "model.fit(training_dataset, epochs = 100, steps_per_epoch=train_steps, validation_data=val_dataset, validation_steps=val_steps,  callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model.h5')"
   ]
  }
 ]
}