% !TEX root = template.tex

\section{Introduction}
\label{sec:introduction}
Recognition of the activity that a user is currently doing is important not only for health care monitoring or security concerns, but also for developing mobile apps that are able to use this information to improve our day life.

Developing mobile apps that are capable of tracking user activities that are used "in the wild" contexts, leads important challenges that need to be tackle down. As stated in \cite{blunck2013heterogeneity} many variables are involved both coming from users and smartphones. Users are demographically different (age, stature, weight, ...) and perform activities in different ways, using their devices in their own ways. Devices instead share among them different operating-systems, hardware and sensing capabilities.
I a recent work \cite{chen2020deep}, authors analyze state-of-the-art deep learning methods for sensor-based HAR, highlighting the challenges in HAR based on their reasons and analyze how existing deep methods are adopted to address novel challenges. Some of these includes for example, recognizing concurrent and composite activities and dealing with scarse annotated, class imbalanced and scarse user distributed datasets, meaning that this task could be very challenging and it require more studies to improve HAR systems.

In this paper we try to perform HAR with smartphone sensors in real use case scenarios, where mobile phones can be hold inside trousers pockets or in a pouch with different orientations, or more simply in a hand. This is particularly relevant when developing mobile apps capable of performing HAR, to improve users quality of life. We proposed a CNN architecture augmented with features extracted from an auto-encoder, and then compared the results with other works. Furthermore we investigate the main benefits of a novel pre-processing pipeline, like orientation independent transformation block that remove the HAR impairments when smartphone are used in different orientation and positions. With this work we aim to study heterogeinity impairments in real use case scenarion in previous state-of-the-art-works, and then proposed a novel learning framework that can be used in developing mobile apps for HAR tasks.

Our main contributions with that work are:
\begin{itemize}
	\item We proposed an architecture that combines CNN and Autoencoder automatic feature extraction to perform HAR. In particular, augmenting CNN with autoencoder features rather than manual features, lead us to better performances results.
	\item We study the importance of a good preprocessing pipeline, to mitigate heterogeneity when dealing with multiple types of smartphone. Also, orientation independent trasform can give promising results in real use case scenarios where smartphones can be in different positions and orientation.
	\item We show that with our proposed model we achieved better results on previous state-of-the-art works, without loosing the main benefits of real-time classification when dealing in environment with few computational resources as smartphones.
\end{itemize}

This paper is structured as follows. In Section II we describe the state of the art, the system and data models are respectively presented in Sections III and IV. The proposed signal processing technique is detailed in Section V and its performance evaluation is carried out in Section VI. Concluding remarks are provided in Section VII. 





%Conclusions:  So to mitigate all these problem when developing Human Activity Recognition (HAR) Systems a representative dataset of all the possibly end-users of out mobile app is required and other pre-processing technique are required for mitigating the heterogenity among devices.

