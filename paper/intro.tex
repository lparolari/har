% !TEX root = template.tex

\section{Introduction}
\label{sec:introduction}

Recognition of the activity that a user is currently doing is
important not only for health care monitoring or security concerns,
but also for developing mobile apps that are able to use this
information to improve our day life.

Developing mobile apps that are capable to track user activities \textit{``in-the-wild''} contexts leads relevant challenges that need to be tackle
down. As stated in \cite{blunck2013heterogeneity} many variables are
involved both coming from users and smartphones. Users are
demographically different (age, stature, weight, ...) and perform
activities in different ways, using their device in their own
style. Devices instead, share among them different operating-systems,
hardware and sensing capabilities.

In a recent work by K. Chen et al. \cite{chen2020deep}, the authors analyze state-of-the-art
deep learning methods for sensor-based HAR, highlighting challenges for HAR tasks, based on their reasons and analyze how existing deep
methods are adopted to address novel challenges. Some of these
includes for example, recognizing concurrent and composite activities
and dealing with scarse annotated, class imbalanced and scarse user
distributed datasets.

In this paper we try to perform HAR with smartphone sensors in real
use case scenarios, where mobile phones can be hold inside trousers
pockets or in a pouch with different orientations, or more simply in a
hand. This is particularly relevant when developing mobile apps
capable of performing HAR, to improve users quality of life. We
propose a CNN architecture augmented with features extracted from an
autoencoder, and then compare the results with other
works. Furthermore we investigate the main benefits of a well thought-out
pre-processing pipeline, like orientation independent transformation
block that remove the HAR impairments when smartphone are used in
different orientation and positions. With this work we aim to study
heterogeneity impairments in real use case scenarios using previous
state-of-the-art models, and then proposed a novel learning framework
that can be used in developing mobile apps for HAR tasks.

Our main contributions can be summarized as follows.

\begin{itemize}
  \item We propose an architecture that combines CNN and automatic
    feature extraction to perform HAR. In particular, augmenting a
    traditional CNN model with autoencoder's features rather than
    manual features can lead to better results.
  \item We study the importance of a good preprocessing pipeline, to
    mitigate heterogeneity when dealing with multiple types of
    smartphones. Also, orientation independent transformation can give
    promising results in real use case scenarios where smartphones can
    be in any position and orientation.
  \item We show that good results can be obtained
    w.r.t. state-of-the-art works also when few computational
    resources are available, i.e, smartphones.
\end{itemize}

This paper is structured as follows. In Section \ref{sec:related-work}
we describe the state-of-the-art, the system and data models are
respectively presented in Sections \ref{sec:processing-pipeline} and
\ref{sec:signals-and-features}. The proposed signal processing
technique is detailed in Section \ref{sec:learning-framework} and its
performance evaluation is carried out in Section
\ref{sec:results}. Concluding remarks are provided in Section
\ref{sec:concluding-remarks}.

%Conclusions:  So to mitigate all these problem when developing Human Activity Recognition (HAR) Systems a representative dataset of all the possibly end-users of out mobile app is required and other pre-processing technique are required for mitigating the heterogenity among devices.
