% !TEX root = template.tex

\section{Concluding Remarks}
\label{sec:concluding-remarks}

In this paper we proposed our solution to perform \textit{``in-the-wild''} HAR, that could be implemented for example, in fitness mobile apps. Dealing with major types of heterogeneity and real use case scenarios, where smartphone could be positioned and oriented in any way, can be a difficult task. Using a good preprocessing pipeline, that implements linear interpolation, orientation independent transformation and data centering, can mitigate HAR impairments due to these latter problems. To perform HAR, various techniques were proposed in literature: one of them very promising is the use of CNN to exploit automatic feature extraction and then compute the final classification. Augmenting CNN with more robust features like the one coming from the encoder part of an autoencoder, we were able to outperform previous results in the original work, that uses the Heterogeneity Dataset. Moreover we demonstrate that orientation independent transform is essential, allowing us to use models trained with dataset made in controlled environments in real use case scenario with promising results. 

In future works it could be useful to test the performance of this model with a more challenging dataset that includes: various types of persons, i.e with different weights, heights, ethnicity, etc. and various type of routes where activities are performed. Moreover it could be useful to test it with subjects wearing different types of shoes and clothes. We would like also to extend the recognizable activities and also includes places were a user could be, i.e be on a car, on a bus, on a train, on a plane etc. Another useful contribution, that due to lack of time we weren't able to test, is the use of open set classification techniques in which the system should reject unknown/unseen activities. Using a smartphone app, where our model was implemented, we saw that the model is constantly trying to predict the learned activities even if the user is completely doing other things with his smartphone.

With this work we learn that datasets are very important when dealing with machine learning / deep learning applications: a huge and good representative dataset of the final use case scenario onto which the model will be used, is extremely important. Moreover, preprocessing techniques are very powerful: finding the right one for the project could lead to completely different results, without changing anything on the final model architecture. Furthermore, Autoencoder automatic feature extraction could be very powerful to augment other popular deep learning techniques.

During our project work we encounter also many problems, one of them was the bad optimization NVIDIA drivers for Ubuntu OS. The machine onto which we were performing our tests, very often crashes due to bad GPU's memory management. Also we encounter a very annoying bug using the pre-fetching Tensorflow Dataset feature, which provide to us very different model metrics results when training the model multiple times with the same hyper-parameters. After disabling that option the system provide us comparable results when dealing with same hyper-parameters. Also we have to report that some of the paper that we read for our project were poorly written, in particular authors were not clear to present their proposed architecture. Sometimes they didn't make explicit model hyper-parameters and training settings, making it difficult to replicate their work.
