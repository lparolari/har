{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            0         1         2         3         4         5         6    \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n            7         8         9    ...  740  741  742  743  744  745  746  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       747  748  749  \n0      0.0  0.0  0.0  \n1      0.0  0.0  0.0  \n2      0.0  0.0  0.0  \n3      0.0  0.0  0.0  \n4      0.0  0.0  0.0  \n...    ...  ...  ...  \n22584  0.0  0.0  0.0  \n22585  0.0  0.0  0.0  \n22586  0.0  0.0  0.0  \n22587  0.0  0.0  0.0  \n22588  0.0  0.0  0.0  \n\n[22589 rows x 750 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_df = pd.read_csv('./datasets/heterogenity/original/dataset_50_2.5.csv', header=None)\n",
    "\n",
    "print(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      user     model     gt\n0        a  nexus4_1  stand\n1        a  nexus4_1  stand\n2        a  nexus4_1  stand\n3        a  nexus4_1  stand\n4        a  nexus4_1  stand\n...    ...       ...    ...\n22584    i  s3mini_2   walk\n22585    i  s3mini_2   walk\n22586    i  s3mini_2   walk\n22587    i  s3mini_2   walk\n22588    i  s3mini_2   walk\n\n[22589 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "label_df = pd.read_csv('./datasets/heterogenity/original/dataset_labels_50_2.5.csv', names=[\"user\", \"model\", \"gt\"])\n",
    "print(label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              0         1         2         3         4         5         6  \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n              7         8         9  ...  743  744  745  746  747  748  749  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       user     model     gt  \n0         a  nexus4_1  stand  \n1         a  nexus4_1  stand  \n2         a  nexus4_1  stand  \n3         a  nexus4_1  stand  \n4         a  nexus4_1  stand  \n...     ...       ...    ...  \n22584     i  s3mini_2   walk  \n22585     i  s3mini_2   walk  \n22586     i  s3mini_2   walk  \n22587     i  s3mini_2   walk  \n22588     i  s3mini_2   walk  \n\n[22589 rows x 753 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.concat([data_df,label_df], axis=1)\n",
    "print(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              0         1         2         3         4         5         6  \\\n0     -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n1     -5.056885 -6.000000 -6.000000 -6.000000 -6.000000 -5.172044 -6.000000   \n2     -5.908984 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000 -6.000000   \n3     -6.000000 -6.000000 -5.283560 -5.862354 -6.000000 -6.000000 -5.063055   \n4     -5.982080 -5.695964 -6.000000 -6.000000 -6.000000 -5.510071 -6.000000   \n...         ...       ...       ...       ...       ...       ...       ...   \n22584 -2.313599 -2.000000 -2.000000 -2.000000 -2.000000 -1.000000 -1.000000   \n22585 -1.493705 -1.000000 -1.000000  0.000000  0.000000  0.000000  0.000000   \n22586  0.000000  0.000000  0.000000  0.000000 -1.000000 -1.000000  0.000000   \n22587  0.000000  0.000000  0.000000  0.720006  0.707711 -0.721697 -4.587960   \n22588 -4.646136 -5.000000 -4.175241 -3.000000 -3.000000 -3.000000 -2.171858   \n\n              7         8         9  ...  748  749  user     model  gt_bike  \\\n0     -6.000000 -6.000000 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n1     -6.000000 -6.000000 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n2     -5.000000 -5.566284 -5.000000  ...  0.0  0.0     a  nexus4_1        0   \n3     -6.000000 -5.242467 -6.000000  ...  0.0  0.0     a  nexus4_1        0   \n4     -6.000000 -6.000000 -5.634155  ...  0.0  0.0     a  nexus4_1        0   \n...         ...       ...       ...  ...  ...  ...   ...       ...      ...   \n22584 -1.000000 -0.306832  0.000000  ...  0.0  0.0     i  s3mini_2        0   \n22585  0.000000  0.000000  0.000000  ...  0.0  0.0     i  s3mini_2        0   \n22586  0.000000  1.000000  0.380511  ...  0.0  0.0     i  s3mini_2        0   \n22587 -6.000000 -4.724040 -3.346348  ...  0.0  0.0     i  s3mini_2        0   \n22588 -1.599592 -1.829834 -2.000000  ...  0.0  0.0     i  s3mini_2        0   \n\n       gt_sit  gt_stairsdown  gt_stairsup  gt_stand  gt_walk  \n0           0              0            0         1        0  \n1           0              0            0         1        0  \n2           0              0            0         1        0  \n3           0              0            0         1        0  \n4           0              0            0         1        0  \n...       ...            ...          ...       ...      ...  \n22584       0              0            0         0        1  \n22585       0              0            0         0        1  \n22586       0              0            0         0        1  \n22587       0              0            0         0        1  \n22588       0              0            0         0        1  \n\n[22589 rows x 758 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_one_hot_df = pd.get_dummies(dataset_df, columns=['gt'])\n",
    "print(dataset_one_hot_df)"
   ]
  },
  {
   "source": [
    "## SPLITTING DATASET IN TRAIN AND TEST SET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN SET\n\tSit:\t\t3694 (20.44%)\n\tStand:\t\t2968 (16.42%)\n\tWalk:\t\t3991 (22.09%)\n\tBike:\t\t2533 (14.02%)\n\tStairs up:\t2735 (15.13%)\n\tStairs down:\t2150 (11.90%)\nVALIDATION SET\n\tSit:\t\t936 (20.72%)\n\tStand:\t\t736 (16.29%)\n\tWalk:\t\t1018 (22.53%)\n\tBike:\t\t623 (13.79%)\n\tStairs up:\t686 (15.18%)\n\tStairs down:\t519 (11.49%)\n"
     ]
    }
   ],
   "source": [
    "# Keep 20% of the data out for validation\n",
    "### START CODE HERE ### (1 line)\n",
    "train_reference_df, val_reference_df = train_test_split(dataset_one_hot_df, test_size=0.2, shuffle=True, random_state=123)\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "def print_dataset_statistics(train_reference_df, val_reference_df):\n",
    "\n",
    "    # Count the elements in the sets\n",
    "    num_train_data_sit = sum(train_reference_df['gt_sit'] == 1)\n",
    "    num_train_data_stand = sum(train_reference_df['gt_stand'] == 1)\n",
    "    num_train_data_walk = sum(train_reference_df['gt_walk'] == 1)\n",
    "    num_train_data_bike = sum(train_reference_df['gt_bike'] == 1)\n",
    "    num_train_data_stairs_up = sum(train_reference_df['gt_stairsup'] == 1)\n",
    "    num_train_data_stairs_down = sum(train_reference_df['gt_stairsdown'] == 1)\n",
    "\n",
    "    num_val_data_sit = sum(val_reference_df['gt_sit'] == 1)\n",
    "    num_val_data_stand = sum(val_reference_df['gt_stand'] == 1)\n",
    "    num_val_data_walk = sum(val_reference_df['gt_walk'] == 1)\n",
    "    num_val_data_bike = sum(val_reference_df['gt_bike'] == 1)\n",
    "    num_val_data_stairs_up = sum(val_reference_df['gt_stairsup'] == 1)\n",
    "    num_val_data_stairs_down = sum(val_reference_df['gt_stairsdown'] == 1)\n",
    "\n",
    "\n",
    "\n",
    "    print('TRAIN SET')\n",
    "    print('\\tSit:\\t\\t{} ({:.2f}%)'.format(num_train_data_sit, 100 * num_train_data_sit / len(train_reference_df)))\n",
    "    print('\\tStand:\\t\\t{} ({:.2f}%)'.format(num_train_data_stand, 100 * num_train_data_stand / len(train_reference_df)))\n",
    "    print('\\tWalk:\\t\\t{} ({:.2f}%)'.format(num_train_data_walk, 100 * num_train_data_walk / len(train_reference_df)))\n",
    "    print('\\tBike:\\t\\t{} ({:.2f}%)'.format(num_train_data_bike, 100 * num_train_data_bike / len(train_reference_df)))\n",
    "    print('\\tStairs up:\\t{} ({:.2f}%)'.format(num_train_data_stairs_up, 100 * num_train_data_stairs_up / len(train_reference_df)))\n",
    "    print('\\tStairs down:\\t{} ({:.2f}%)'.format(num_train_data_stairs_down, 100 * num_train_data_stairs_down / len(train_reference_df)))\n",
    "\n",
    "\n",
    "    print('VALIDATION SET')\n",
    "    print('\\tSit:\\t\\t{} ({:.2f}%)'.format(num_val_data_sit, 100 * num_val_data_sit / len(val_reference_df)))\n",
    "    print('\\tStand:\\t\\t{} ({:.2f}%)'.format(num_val_data_stand, 100 * num_val_data_stand / len(val_reference_df)))\n",
    "    print('\\tWalk:\\t\\t{} ({:.2f}%)'.format(num_val_data_walk, 100 * num_val_data_walk / len(val_reference_df)))\n",
    "    print('\\tBike:\\t\\t{} ({:.2f}%)'.format(num_val_data_bike, 100 * num_val_data_bike / len(val_reference_df)))\n",
    "    print('\\tStairs up:\\t{} ({:.2f}%)'.format(num_val_data_stairs_up, 100 * num_val_data_stairs_up / len(val_reference_df)))\n",
    "    print('\\tStairs down:\\t{} ({:.2f}%)'.format(num_val_data_stairs_down, 100 * num_val_data_stairs_down / len(val_reference_df)))\n",
    "\n",
    "\n",
    "print_dataset_statistics(train_reference_df, val_reference_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         0         1    2        3         4    5         6         7  \\\n6785  -3.0 -3.000000 -3.0 -3.00000 -2.091919 -2.0 -2.829190 -3.000000   \n20108  5.0  5.000000  5.0  5.00000  5.000000  5.0  5.000000  5.000000   \n17854 -5.0 -4.733199 -4.0 -4.00000 -4.000000 -3.0 -3.000000 -3.214111   \n16603  4.0  4.000000  4.0  4.00000  4.000000  4.0  4.000000  4.000000   \n14823 -1.0 -1.000000 -1.0 -1.00000 -1.000000 -1.0 -1.153836 -2.000000   \n...    ...       ...  ...      ...       ...  ...       ...       ...   \n15377 -1.0 -1.000000 -1.0 -1.00000 -1.000000 -1.0 -1.000000 -1.000000   \n21602  5.0  5.000000  5.0  5.00000  5.000000  5.0  5.000000  5.000000   \n17730 -5.0 -5.000000 -5.0 -4.45692 -4.000000 -3.0 -3.998675 -4.000000   \n15725 -1.0 -1.000000 -1.0 -2.00000 -5.176727 -6.0 -7.000000 -5.000000   \n19966  0.0  0.000000  0.0  0.00000  0.000000  0.0  0.000000  0.000000   \n\n              8         9  ...  748  749  user     model  gt_bike  gt_sit  \\\n6785  -4.000000 -4.818164  ...  0.0  0.0     c      s3_2        0       0   \n20108  5.000000  5.000000  ...  0.0  0.0     i  nexus4_1        0       1   \n17854 -3.000000 -3.000000  ...  0.0  0.0     h  nexus4_1        0       0   \n16603  4.000000  4.000000  ...  0.0  0.0     g      s3_2        0       1   \n14823 -1.879435 -1.000000  ...  0.0  0.0     f  s3mini_1        1       0   \n...         ...       ...  ...  ...  ...   ...       ...      ...     ...   \n15377 -1.000000 -1.000000  ...  0.0  0.0     g  nexus4_2        0       0   \n21602  5.000000  5.000000  ...  0.0  0.0     i      s3_2        0       1   \n17730 -4.000000 -3.460956  ...  0.0  0.0     h  nexus4_1        0       0   \n15725 -4.000000 -4.000000  ...  0.0  0.0     g  nexus4_2        0       0   \n19966  0.000000  0.000000  ...  0.0  0.0     i  nexus4_1        0       0   \n\n       gt_stairsdown  gt_stairsup  gt_stand  gt_walk  \n6785               0            0         0        1  \n20108              0            0         0        0  \n17854              1            0         0        0  \n16603              0            0         0        0  \n14823              0            0         0        0  \n...              ...          ...       ...      ...  \n15377              0            0         1        0  \n21602              0            0         0        0  \n17730              0            0         0        1  \n15725              0            1         0        0  \n19966              0            0         1        0  \n\n[18071 rows x 758 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_reference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 6.3         5.6         5.6         2.9         2.76405499  2.76405499\n   2.5         2.4         2.4        10.26614733  0.5         0.\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         1.          0.5         0.          1.          0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.          1.          0.5         0.5         0.5\n   0.5         0.5         0.5         0.5       ]\n [ 5.5         5.5         5.5         2.87228132  2.87228132  2.87228132\n   2.5         2.5         2.5         9.52627944  0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_basic_features(acc_x, acc_y, acc_z):\n",
    "    # prova = np.array(np.apply_along_axis(np.histogram, 1, acc_x)[0]).reshape(2,1)\n",
    "\n",
    "    np_acc_x = np.array(acc_x)\n",
    "    np_acc_y = np.array(acc_y)\n",
    "    np_acc_z = np.array(acc_z)\n",
    "\n",
    "    mean_x = np.expand_dims(np.mean(np_acc_x, axis=1), axis=0).T \n",
    "    mean_y = np.expand_dims(np.mean(np_acc_y, axis=1), axis=0).T \n",
    "    mean_z = np.expand_dims(np.mean(np_acc_z, axis=1), axis=0).T \n",
    "\n",
    "    basic_features = np.concatenate( (\n",
    "        # insert MEANS \n",
    "        mean_x,\n",
    "        mean_y,\n",
    "        mean_z,\n",
    "\n",
    "        # insert STD\n",
    "        np.expand_dims(np.std(np_acc_x, axis=1), axis=0).T, \n",
    "        np.expand_dims(np.std(np_acc_y, axis=1), axis=0).T, \n",
    "        np.expand_dims(np.std(np_acc_z, axis=1), axis=0).T, \n",
    "\n",
    "        # insert sum of thew absolute values\n",
    "        np.expand_dims(np.mean(abs(np_acc_x - mean_x), axis=1), axis=1),\n",
    "        np.expand_dims(np.mean(abs(np_acc_y - mean_y), axis=1), axis=1),\n",
    "        np.expand_dims(np.mean(abs(np_acc_z - mean_z), axis=1), axis=1),\n",
    "\n",
    "        np.expand_dims(np.mean( np.sqrt( np.power(np_acc_x, 2) + np.power(np_acc_y,2) + np.power(np_acc_z, 2) ), axis=1), axis=0).T\n",
    "        \n",
    "    ), axis=1).tolist()\n",
    "\n",
    "    for i in range(0, len(acc_x)):\n",
    "        bins_x, centers_x = np.histogram(acc_x[i], bins=10)\n",
    "        bins_y, centers_y = np.histogram(acc_y[i], bins=10)\n",
    "        bins_z, centers_z = np.histogram(acc_z[i], bins=10)\n",
    "\n",
    "        basic_features[i].extend(bins_x / len(acc_x))\n",
    "        basic_features[i].extend(bins_y / len(acc_y))\n",
    "        basic_features[i].extend(bins_z / len(acc_z))\n",
    "\n",
    "    return basic_features\n",
    "\n",
    "prova_feat = np.array(extract_basic_features( [[1,10,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]] , [[1,3,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]], [[1,3,3,4,5,6,7,8,9,10], [1,2,3,4,5,6,7,8,9,10]]))\n",
    "\n",
    "print(prova_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Features: {'input_1': <tf.Tensor: shape=(128, 6, 125), dtype=float64, numpy=\narray([[[-3.        , -3.        , -3.        , ..., -5.        ,\n         -5.        , -4.        ],\n        [ 0.        ,  0.        ,  0.        , ..., -0.28190104,\n          0.        ,  0.        ],\n        [ 5.        ,  5.        ,  5.90402832, ...,  6.        ,\n          6.64831543,  7.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 5.        ,  5.        ,  5.        , ...,  5.        ,\n          5.        ,  5.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 8.        ,  8.        ,  8.        , ...,  8.        ,\n          8.        ,  8.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[-5.        , -4.73319869, -4.        , ..., -3.        ,\n         -3.        , -3.        ],\n        [ 2.        ,  1.26680131,  2.        , ..., -1.        ,\n         -1.        , -1.        ],\n        [10.        ,  8.        ,  7.33589681, ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       ...,\n\n       [[-4.        , -4.        , -4.        , ..., -4.        ,\n         -4.        , -4.        ],\n        [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 8.        ,  9.        ,  9.        , ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 3.05450439,  4.        ,  3.        , ...,  4.        ,\n          4.        ,  4.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 9.        ,  9.        ,  9.        , ...,  9.        ,\n          9.        ,  9.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]],\n\n       [[ 1.        ,  1.        ,  1.        , ...,  1.        ,\n          1.        ,  1.        ],\n        [-2.        , -2.        , -1.99567871, ..., -2.        ,\n         -1.        , -1.        ],\n        [ 7.        ,  7.        ,  7.        , ...,  7.66396484,\n          8.        ,  7.33168945],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.73662109],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ],\n        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n          0.        ,  0.        ]]])>, 'input_2': <tf.Tensor: shape=(128, 40), dtype=float64, numpy=\narray([[-4.69011002e+00, -8.60144423e-02,  7.68005064e+00, ...,\n         4.42698246e-04,  4.98035527e-04,  2.76686404e-04],\n       [ 5.00000000e+00,  0.00000000e+00,  8.00000000e+00, ...,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [-4.84529689e+00,  1.28745553e-01,  7.87250156e+00, ...,\n         1.10674561e-04,  1.10674561e-04,  5.53372807e-05],\n       ...,\n       [-4.00000000e+00,  5.96103322e-01,  8.74624516e+00, ...,\n         5.53372807e-05,  1.10674561e-04,  4.86968070e-03],\n       [ 3.61243781e+00,  0.00000000e+00,  9.00000000e+00, ...,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n       [ 8.34721609e-01,  2.59604138e-02,  8.88621802e+00, ...,\n         5.53372807e-05,  4.42698246e-04,  1.10674561e-04]])>}, Target: [[0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 1 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [1 0 0 0 0 0]\n [0 0 0 0 1 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 0 0 1 0 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [1 0 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 1 0]\n [1 0 0 0 0 0]\n [0 0 0 0 0 1]\n [0 0 0 0 0 1]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 1 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 1 0]\n [0 0 0 0 0 1]\n [0 0 0 1 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [1 0 0 0 0 0]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [0 0 0 0 0 1]\n [0 1 0 0 0 0]\n [0 1 0 0 0 0]\n [1 0 0 0 0 0]\n [0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(reference_df, batch_size, shuffle, cache_file, center_data=False):\n",
    "    target = reference_df[['gt_sit','gt_stand','gt_walk','gt_bike','gt_stairsup','gt_stairsdown']].values.astype(int).tolist()\n",
    "\n",
    "    # RESHAPING DATAS\n",
    "    np_data = np.array(reference_df.iloc[:,0:750])\n",
    "\n",
    "    np_reshaped_data = np.reshape(np_data.copy(), (np_data.shape[0], 6, 125))\n",
    "\n",
    "    # Data centering\n",
    "    if center_data:\n",
    "        for i in range(len(np_reshaped_data)):\n",
    "            window = np_reshaped_data[i]\n",
    "            means = np.mean(window, axis=1)\n",
    "            centered_acc = np.array(([window[j] - means[j] for j in range(3)]))\n",
    "            np_reshaped_data[i] = np.concatenate((centered_acc, window[3:]), axis=0)\n",
    "             \n",
    "    \n",
    "    # Extract manual features\n",
    "    np_basic_features = np.array(extract_basic_features(np_data[:, 0:125], np_data[:, 125:250], np_data[:, 250: 375]))\n",
    "\n",
    "    # Create dataset obj\n",
    "    dataset = tf.data.Dataset.from_tensor_slices( ({\"input_1\": np_reshaped_data, \"input_2\": np_basic_features}, target) )\n",
    "\n",
    "    # Cache dataset\n",
    "    if cache_file:\n",
    "        dataset = dataset.cache(cache_file)\n",
    "\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(target))\n",
    "\n",
    "    # Repeat the dataset indefinitely\n",
    "    dataset = dataset.repeat()\n",
    "\n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "    # Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "training_dataset = create_dataset(train_reference_df, batch_size=batch_size, shuffle=False, cache_file=None)\n",
    "val_dataset = create_dataset(val_reference_df, batch_size=batch_size, shuffle=False, cache_file=None)\n",
    "\n",
    "for train, targ in training_dataset.take(1):\n",
    "  print ('Features: {}, Target: {}'.format(train, targ))\n",
    "\n",
    "train_steps = int(np.ceil(len(train_reference_df)/batch_size))\n",
    "val_steps = int(np.ceil(len(val_reference_df)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"OurModel\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6, 125)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 6, 196)       392196      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 2, 196)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 392)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_3 (TensorFlo [(None, 432)]        0           flatten_3[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         443392      tf_op_layer_concat_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 6)            6150        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 841,738\n",
      "Trainable params: 841,738\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_model(input_shape):\n",
    "\n",
    "    l2_reg = 5e-4\n",
    "\n",
    "    encoder = tf.keras.models.load_model('encoder.h5')\n",
    "\n",
    "    # NOT TRAIN THE MODEL\n",
    "    encoder.trainable = False\n",
    "\n",
    "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    training_input = tf.keras.Input(shape=input_shape, dtype=tf.float32, name='input_1')\n",
    "    basic_feat_input = tf.keras.Input(shape=40, dtype=tf.float32, name='input_2')\n",
    "\n",
    "    CNN = tf.keras.layers.Conv1D(196, 16, activation='relu', padding='same')(training_input)\n",
    "    CNN = tf.keras.layers.MaxPool1D(4, padding='same')(CNN)\n",
    "    \n",
    "    feautures_CCN = tf.keras.layers.Flatten()(CNN)\n",
    "    \n",
    "    featuers_encoder = encoder(training_input)\n",
    "\n",
    "    features = tf.concat((feautures_CCN, basic_feat_input), 1) \n",
    "\n",
    "    #features = tf.concat((feautures_CCN), 1) \n",
    "\n",
    "    FFNN = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2_reg), activity_regularizer=tf.keras.regularizers.L2(l2_reg))(features)\n",
    "    FFNN = tf.keras.layers.Dropout(0.05)(FFNN)\n",
    "    model_output = tf.keras.layers.Dense(6, activation='softmax')(FFNN)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [training_input, basic_feat_input], outputs = model_output, name='OurModel')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model((6,125))\n",
    "\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "loss_funct = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer = adam_optimizer, loss = loss_funct, metrics = [\"accuracy\"])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "137/142 [===========================>..] - ETA: 0s - loss: 1.2994 - accuracy: 0.7107WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0027s vs `on_test_batch_end` time: 0.0053s). Check your callbacks.\n",
      "142/142 [==============================] - 1s 7ms/step - loss: 1.2834 - accuracy: 0.7148 - val_loss: 0.7778 - val_accuracy: 0.8602\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.6692 - accuracy: 0.8818 - val_loss: 0.5920 - val_accuracy: 0.9206\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.5255 - accuracy: 0.9191 - val_loss: 0.4894 - val_accuracy: 0.9319\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.4491 - accuracy: 0.9376 - val_loss: 0.4364 - val_accuracy: 0.9373\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.3935 - accuracy: 0.9476 - val_loss: 0.3965 - val_accuracy: 0.9438\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 1s 7ms/step - loss: 0.3533 - accuracy: 0.9551 - val_loss: 0.3684 - val_accuracy: 0.9431\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.3241 - accuracy: 0.9593 - val_loss: 0.3514 - val_accuracy: 0.9442\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.3050 - accuracy: 0.9601 - val_loss: 0.3390 - val_accuracy: 0.9457\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.2759 - accuracy: 0.9660 - val_loss: 0.3307 - val_accuracy: 0.9421\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.2523 - accuracy: 0.9680 - val_loss: 0.3119 - val_accuracy: 0.9453\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.2354 - accuracy: 0.9694 - val_loss: 0.2824 - val_accuracy: 0.9501\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.2196 - accuracy: 0.9717 - val_loss: 0.2587 - val_accuracy: 0.9555\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.2062 - accuracy: 0.9725 - val_loss: 0.2535 - val_accuracy: 0.9540\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1942 - accuracy: 0.9724 - val_loss: 0.2359 - val_accuracy: 0.9553\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1862 - accuracy: 0.9724 - val_loss: 0.2458 - val_accuracy: 0.9533\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1785 - accuracy: 0.9714 - val_loss: 0.2328 - val_accuracy: 0.9533\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1683 - accuracy: 0.9728 - val_loss: 0.2170 - val_accuracy: 0.9551\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1637 - accuracy: 0.9715 - val_loss: 0.2618 - val_accuracy: 0.9412\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1739 - accuracy: 0.9670 - val_loss: 0.2291 - val_accuracy: 0.9516\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1496 - accuracy: 0.9731 - val_loss: 0.2141 - val_accuracy: 0.9570\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1437 - accuracy: 0.9728 - val_loss: 0.2068 - val_accuracy: 0.9559\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1357 - accuracy: 0.9739 - val_loss: 0.1925 - val_accuracy: 0.9575\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1287 - accuracy: 0.9747 - val_loss: 0.1933 - val_accuracy: 0.9570\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1206 - accuracy: 0.9757 - val_loss: 0.2070 - val_accuracy: 0.9529\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1166 - accuracy: 0.9752 - val_loss: 0.2088 - val_accuracy: 0.9494\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 1s 7ms/step - loss: 0.1120 - accuracy: 0.9752 - val_loss: 0.2563 - val_accuracy: 0.9347\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 1s 7ms/step - loss: 0.1175 - accuracy: 0.9726 - val_loss: 0.1929 - val_accuracy: 0.9492\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 1s 7ms/step - loss: 0.1110 - accuracy: 0.9735 - val_loss: 0.1962 - val_accuracy: 0.9486\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1052 - accuracy: 0.9751 - val_loss: 0.1799 - val_accuracy: 0.9579\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.0960 - accuracy: 0.9766 - val_loss: 0.1573 - val_accuracy: 0.9607\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.0906 - accuracy: 0.9762 - val_loss: 0.1538 - val_accuracy: 0.9614\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.0881 - accuracy: 0.9769 - val_loss: 0.1750 - val_accuracy: 0.9501\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 1s 7ms/step - loss: 0.1051 - accuracy: 0.9712 - val_loss: 0.2321 - val_accuracy: 0.9362\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1060 - accuracy: 0.9704 - val_loss: 0.1864 - val_accuracy: 0.9484\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 1s 6ms/step - loss: 0.1075 - accuracy: 0.9691 - val_loss: 0.1668 - val_accuracy: 0.9555\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f05167b3cd0>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "model.fit(training_dataset, epochs = 100, steps_per_epoch=train_steps, validation_data=val_dataset, validation_steps=val_steps,  callbacks = [early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/model.h5')"
   ]
  }
 ]
}